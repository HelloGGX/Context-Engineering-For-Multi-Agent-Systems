import os
from openai import OpenAI
from pinecone import Pinecone
from tenacity import retry, stop_after_attempt, wait_random_exponential
import tiktoken
from registry import AppConfig
from tqdm import tqdm

tokenizer = tiktoken.get_encoding("cl100k_base")
config = AppConfig()
client = OpenAI(base_url=config.base_url, api_key=config.api_key)
pc = Pinecone(api_key=config.pinecone_api_key)
index = pc.Index(config.index_name)


def chunk_text(text, chunk_size=400, overlap=50):
    tokens = tokenizer.encode(text)
    chunks = []  # å­˜å‚¨æœ€ç»ˆæ‹†åˆ†çš„æ–‡æœ¬å—
    # å¾ªç¯é€»è¾‘ï¼šä»tokenåºåˆ—çš„0ä½ç½®å¼€å§‹ï¼ŒæŒ‰â€œæ­¥é•¿=chunk_size - overlapâ€è¿­ä»£
    for i in range(0, len(tokens), chunk_size - overlap):
        # 1. æˆªå–å½“å‰å—çš„tokenï¼ˆä»iåˆ°i+chunk_sizeï¼Œé¿å…è¶…å‡ºæ€»é•¿åº¦ï¼‰
        chunk_tokens = tokens[i : i + chunk_size]
        # 2. Tokenâ†’æ–‡æœ¬è§£ç ï¼ˆå°†tokenåºåˆ—è¿˜åŸä¸ºäººç±»å¯è¯»çš„æ–‡æœ¬ï¼‰
        chunk_text = tokenizer.decode(chunk_tokens)
        # 3. åŸºç¡€æ¸…ç†ï¼ˆé¿å…ç©ºè¡Œã€å¤šä½™ç©ºæ ¼å½±å“æ£€ç´¢ç²¾åº¦ï¼‰
        chunk_text = chunk_text.replace("\n", " ").strip()  # æ¢è¡Œç¬¦â†’ç©ºæ ¼ï¼Œå»é™¤é¦–å°¾ç©ºç™½
        # 4. è¿‡æ»¤ç©ºå—ï¼ˆé¿å…å› æ‹†åˆ†é€»è¾‘äº§ç”Ÿçš„ç©ºæ–‡æœ¬ï¼‰
        if chunk_text:
            chunks.append(chunk_text)

    return chunks


@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
def get_embeddings_batch(texts, model=config.embedding_model):
    """Generates embeddings for a batch of texts using OpenAI, with retries."""
    # OpenAI expects the input texts to have newlines replaced by spaces
    texts = [t.replace("\n", " ") for t in texts]
    response = client.embeddings.create(
        input=texts, model=model, dimensions=config.embedding_dim
    )
    return [item.embedding for item in response.data]


if not os.path.exists("nasa_documents"):
    os.makedirs("nasa_documents")

knowledge_base = {}
doc_dir = "nasa_documents"
for filename in os.listdir(doc_dir):
    if filename.endswith(".txt"):
        with open(os.path.join(doc_dir, filename), "r") as f:
            knowledge_base[filename] = f.read()

print(
    f"ğŸ“š Loaded {len(knowledge_base)} documents into the knowledge base."
)  # We use sample data related to space exploration.

print(f"\nProcessing and uploading Context Library to namespace: {config.ns_knowledge}")

batch_size = 100
total_vectors_uploaded = 0

for doc_name, doc_content in knowledge_base.items():
    knowledge_chunks = chunk_text(doc_content)

    for i in tqdm(
        range(0, len(knowledge_chunks), batch_size), desc=f" Uploading {doc_name}"
    ):
        batch_texts = knowledge_chunks[i : i + batch_size]
        batch_embeddings = get_embeddings_batch(batch_texts)
        batch_vectors = []
        for j, embedding in enumerate(batch_embeddings):
            chunk_id = f"{doc_name}_chunk_{total_vectors_uploaded + j}"

            batch_vectors.append(
                {
                    "id": chunk_id,
                    "values": embedding,
                    "metadata": {"text": batch_texts[j], "source": doc_name},
                }
            )
        index.upsert(vectors=batch_vectors, namespace=config.ns_knowledge)

    total_vectors_uploaded += len(knowledge_chunks)

print(
    f"\nâœ… Successfully uploaded {total_vectors_uploaded} knowledge vectors from {len(knowledge_base)} documents."
)
